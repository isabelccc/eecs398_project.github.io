{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Understanding and Predicting Power Outage Patterns in the United States\"\n",
    "\n",
    "**Name(s)**: xiulin chen\n",
    "\n",
    "**Website Link**: https://isabelccc.github.io/eecs398_project.github.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['variables', 'OBS', 'YEAR', 'MONTH', 'U.S._STATE', 'POSTAL.CODE',\n",
      "       'NERC.REGION', 'CLIMATE.REGION', 'ANOMALY.LEVEL', 'CLIMATE.CATEGORY',\n",
      "       'OUTAGE.START.DATE', 'OUTAGE.START.TIME', 'OUTAGE.RESTORATION.DATE',\n",
      "       'OUTAGE.RESTORATION.TIME', 'CAUSE.CATEGORY', 'CAUSE.CATEGORY.DETAIL',\n",
      "       'HURRICANE.NAMES', 'OUTAGE.DURATION', 'DEMAND.LOSS.MW',\n",
      "       'CUSTOMERS.AFFECTED', 'RES.PRICE', 'COM.PRICE', 'IND.PRICE',\n",
      "       'TOTAL.PRICE', 'RES.SALES', 'COM.SALES', 'IND.SALES', 'TOTAL.SALES',\n",
      "       'RES.PERCEN', 'COM.PERCEN', 'IND.PERCEN', 'RES.CUSTOMERS',\n",
      "       'COM.CUSTOMERS', 'IND.CUSTOMERS', 'TOTAL.CUSTOMERS', 'RES.CUST.PCT',\n",
      "       'COM.CUST.PCT', 'IND.CUST.PCT', 'PC.REALGSP.STATE', 'PC.REALGSP.USA',\n",
      "       'PC.REALGSP.REL', 'PC.REALGSP.CHANGE', 'UTIL.REALGSP', 'TOTAL.REALGSP',\n",
      "       'UTIL.CONTRI', 'PI.UTIL.OFUSA', 'POPULATION', 'POPPCT_URBAN',\n",
      "       'POPPCT_UC', 'POPDEN_URBAN', 'POPDEN_UC', 'POPDEN_RURAL',\n",
      "       'AREAPCT_URBAN', 'AREAPCT_UC', 'PCT_LAND', 'PCT_WATER_TOT',\n",
      "       'PCT_WATER_INLAND'],\n",
      "      dtype='object')\n",
      "Rows: 1535\n",
      "Columns: ['variables', 'OBS', 'YEAR', 'MONTH', 'U.S._STATE', 'POSTAL.CODE', 'NERC.REGION', 'CLIMATE.REGION', 'ANOMALY.LEVEL', 'CLIMATE.CATEGORY', 'OUTAGE.START.DATE', 'OUTAGE.START.TIME', 'OUTAGE.RESTORATION.DATE', 'OUTAGE.RESTORATION.TIME', 'CAUSE.CATEGORY', 'CAUSE.CATEGORY.DETAIL', 'HURRICANE.NAMES', 'OUTAGE.DURATION', 'DEMAND.LOSS.MW', 'CUSTOMERS.AFFECTED', 'RES.PRICE', 'COM.PRICE', 'IND.PRICE', 'TOTAL.PRICE', 'RES.SALES', 'COM.SALES', 'IND.SALES', 'TOTAL.SALES', 'RES.PERCEN', 'COM.PERCEN', 'IND.PERCEN', 'RES.CUSTOMERS', 'COM.CUSTOMERS', 'IND.CUSTOMERS', 'TOTAL.CUSTOMERS', 'RES.CUST.PCT', 'COM.CUST.PCT', 'IND.CUST.PCT', 'PC.REALGSP.STATE', 'PC.REALGSP.USA', 'PC.REALGSP.REL', 'PC.REALGSP.CHANGE', 'UTIL.REALGSP', 'TOTAL.REALGSP', 'UTIL.CONTRI', 'PI.UTIL.OFUSA', 'POPULATION', 'POPPCT_URBAN', 'POPPCT_UC', 'POPDEN_URBAN', 'POPDEN_UC', 'POPDEN_RURAL', 'AREAPCT_URBAN', 'AREAPCT_UC', 'PCT_LAND', 'PCT_WATER_TOT', 'PCT_WATER_INLAND']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"outage.xlsx\", header=5)\n",
    "print(df.columns)\n",
    "df.head()\n",
    "\n",
    "print(\"Rows:\", df.shape[0])\n",
    "print(\"Columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:Introduction and Question Identification\n",
    "Our project leverages a comprehensive dataset detailing major power outage events across the United States, spanning from the year 2000 to 2016. Each row in the dataset represents a single outage incident, enriched with contextual features such as climate conditions, regional infrastructure metrics, population data, and outage metadata. The dataset originates from U.S. government and energy infrastructure reports.\n",
    "\n",
    "Total Rows: ~1535\n",
    "\n",
    "\t•\tKey columns for analysis:\n",
    "\t•\tYEAR, MONTH: When the outage happened\n",
    "\t•\tU.S._STATE, CLIMATE.REGION: Where it occurred\n",
    "\t•\tCLIMATE.CATEGORY, ANOMALY.LEVEL: Climate conditions\n",
    "\t•\tCUSTOMERS.AFFECTED: Impact\n",
    "\t•\tOUTAGE.START, OUTAGE.RESTORATION: Timing\n",
    "\t•\tCAUSE.CATEGORY: What caused the outage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Data Cleaning and Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  U.S._STATE        OUTAGE.START  OUTAGE.RESTORATION  DURATION_HOURS\n",
      "1  Minnesota 2011-07-01 17:00:00 2011-07-03 20:00:00            51.0\n",
      "3  Minnesota 2010-10-26 20:00:00 2010-10-28 22:00:00            50.0\n",
      "4  Minnesota 2012-06-19 04:30:00 2012-06-20 23:00:00            42.5\n",
      "5  Minnesota 2015-07-18 02:00:00 2015-07-19 07:00:00            29.0\n",
      "6  Minnesota 2010-11-13 15:00:00 2010-11-14 22:00:00            31.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1056"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['U.S._STATE'].unique()\n",
    "df_valid = df[df[\"U.S._STATE\"].notna()]\n",
    "\n",
    "# Make a copy to avoid SettingWithCopyWarning\n",
    "df_valid = df_valid.copy()\n",
    "\n",
    "# Convert date and time columns to string explicitly\n",
    "df_valid[\"OUTAGE.START.DATE\"] = pd.to_datetime(df_valid[\"OUTAGE.START.DATE\"], errors='coerce').dt.date.astype(str)\n",
    "df_valid[\"OUTAGE.START.TIME\"] = df_valid[\"OUTAGE.START.TIME\"].astype(str)\n",
    "\n",
    "df_valid[\"OUTAGE.RESTORATION.DATE\"] = pd.to_datetime(df_valid[\"OUTAGE.RESTORATION.DATE\"], errors='coerce').dt.date.astype(str)\n",
    "df_valid[\"OUTAGE.RESTORATION.TIME\"] = df_valid[\"OUTAGE.RESTORATION.TIME\"].astype(str)\n",
    "\n",
    "# Combine date and time strings and convert to datetime\n",
    "df_valid[\"OUTAGE.START\"] = pd.to_datetime(\n",
    "    df_valid[\"OUTAGE.START.DATE\"] + \" \" + df_valid[\"OUTAGE.START.TIME\"],\n",
    "    format=\"%Y-%m-%d %H:%M:%S\", errors=\"coerce\"\n",
    ")\n",
    "\n",
    "df_valid[\"OUTAGE.RESTORATION\"] = pd.to_datetime(\n",
    "    df_valid[\"OUTAGE.RESTORATION.DATE\"] + \" \" + df_valid[\"OUTAGE.RESTORATION.TIME\"],\n",
    "    format=\"%Y-%m-%d %H:%M:%S\", errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Calculate outage duration in hours\n",
    "df_valid[\"DURATION_HOURS\"] = (df_valid[\"OUTAGE.RESTORATION\"] - df_valid[\"OUTAGE.START\"]).dt.total_seconds() / 3600\n",
    "\n",
    "# Drop invalid rows\n",
    "df_valid = df_valid.dropna(subset=[\"OUTAGE.START\", \"OUTAGE.RESTORATION\", \"DURATION_HOURS\"])\n",
    "\n",
    "# Ensure numeric columns are cast correctly:\n",
    "df_valid[\"CUSTOMERS.AFFECTED\"] = pd.to_numeric(df_valid[\"CUSTOMERS.AFFECTED\"], errors=\"coerce\")\n",
    "df_valid[\"ANOMALY.LEVEL\"] = pd.to_numeric(df_valid[\"ANOMALY.LEVEL\"], errors=\"coerce\")\n",
    "\n",
    "\n",
    "# Remove duration and customer outliers\n",
    "duration_threshold = df_valid[\"DURATION_HOURS\"].quantile(0.999)\n",
    "df_valid = df_valid[\n",
    "    (df_valid[\"DURATION_HOURS\"] <= duration_threshold) &\n",
    "    (df_valid[\"CUSTOMERS.AFFECTED\"] < 1e7)\n",
    "]\n",
    "\n",
    "# Show result\n",
    "print(df_valid[[\"U.S._STATE\", \"OUTAGE.START\", \"OUTAGE.RESTORATION\", \"DURATION_HOURS\"]].head())\n",
    "df_valid.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>U.S._STATE</th>\n",
       "      <th>CLIMATE.REGION</th>\n",
       "      <th>ANOMALY.LEVEL</th>\n",
       "      <th>CLIMATE.CATEGORY</th>\n",
       "      <th>CUSTOMERS.AFFECTED</th>\n",
       "      <th>POPDEN_URBAN</th>\n",
       "      <th>POPDEN_RURAL</th>\n",
       "      <th>AREAPCT_URBAN</th>\n",
       "      <th>AREAPCT_UC</th>\n",
       "      <th>PCT_LAND</th>\n",
       "      <th>PCT_WATER_TOT</th>\n",
       "      <th>PCT_WATER_INLAND</th>\n",
       "      <th>OUTAGE.START</th>\n",
       "      <th>OUTAGE.RESTORATION</th>\n",
       "      <th>DURATION_HOURS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>East North Central</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>normal</td>\n",
       "      <td>70000.0</td>\n",
       "      <td>2279</td>\n",
       "      <td>18.2</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.6</td>\n",
       "      <td>91.592666</td>\n",
       "      <td>8.407334</td>\n",
       "      <td>5.478743</td>\n",
       "      <td>2011-07-01 17:00:00</td>\n",
       "      <td>2011-07-03 20:00:00</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>East North Central</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>cold</td>\n",
       "      <td>70000.0</td>\n",
       "      <td>2279</td>\n",
       "      <td>18.2</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.6</td>\n",
       "      <td>91.592666</td>\n",
       "      <td>8.407334</td>\n",
       "      <td>5.478743</td>\n",
       "      <td>2010-10-26 20:00:00</td>\n",
       "      <td>2010-10-28 22:00:00</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>East North Central</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>normal</td>\n",
       "      <td>68200.0</td>\n",
       "      <td>2279</td>\n",
       "      <td>18.2</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.6</td>\n",
       "      <td>91.592666</td>\n",
       "      <td>8.407334</td>\n",
       "      <td>5.478743</td>\n",
       "      <td>2012-06-19 04:30:00</td>\n",
       "      <td>2012-06-20 23:00:00</td>\n",
       "      <td>42.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>East North Central</td>\n",
       "      <td>1.2</td>\n",
       "      <td>warm</td>\n",
       "      <td>250000.0</td>\n",
       "      <td>2279</td>\n",
       "      <td>18.2</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.6</td>\n",
       "      <td>91.592666</td>\n",
       "      <td>8.407334</td>\n",
       "      <td>5.478743</td>\n",
       "      <td>2015-07-18 02:00:00</td>\n",
       "      <td>2015-07-19 07:00:00</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2010.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>East North Central</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>cold</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>2279</td>\n",
       "      <td>18.2</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.6</td>\n",
       "      <td>91.592666</td>\n",
       "      <td>8.407334</td>\n",
       "      <td>5.478743</td>\n",
       "      <td>2010-11-13 15:00:00</td>\n",
       "      <td>2010-11-14 22:00:00</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     YEAR  MONTH U.S._STATE      CLIMATE.REGION  ANOMALY.LEVEL  \\\n",
       "1  2011.0    7.0  Minnesota  East North Central           -0.3   \n",
       "3  2010.0   10.0  Minnesota  East North Central           -1.5   \n",
       "4  2012.0    6.0  Minnesota  East North Central           -0.1   \n",
       "5  2015.0    7.0  Minnesota  East North Central            1.2   \n",
       "6  2010.0   11.0  Minnesota  East North Central           -1.4   \n",
       "\n",
       "  CLIMATE.CATEGORY  CUSTOMERS.AFFECTED POPDEN_URBAN POPDEN_RURAL  \\\n",
       "1           normal             70000.0         2279         18.2   \n",
       "3             cold             70000.0         2279         18.2   \n",
       "4           normal             68200.0         2279         18.2   \n",
       "5             warm            250000.0         2279         18.2   \n",
       "6             cold             60000.0         2279         18.2   \n",
       "\n",
       "  AREAPCT_URBAN AREAPCT_UC   PCT_LAND PCT_WATER_TOT PCT_WATER_INLAND  \\\n",
       "1          2.14        0.6  91.592666      8.407334         5.478743   \n",
       "3          2.14        0.6  91.592666      8.407334         5.478743   \n",
       "4          2.14        0.6  91.592666      8.407334         5.478743   \n",
       "5          2.14        0.6  91.592666      8.407334         5.478743   \n",
       "6          2.14        0.6  91.592666      8.407334         5.478743   \n",
       "\n",
       "         OUTAGE.START  OUTAGE.RESTORATION  DURATION_HOURS  \n",
       "1 2011-07-01 17:00:00 2011-07-03 20:00:00            51.0  \n",
       "3 2010-10-26 20:00:00 2010-10-28 22:00:00            50.0  \n",
       "4 2012-06-19 04:30:00 2012-06-20 23:00:00            42.5  \n",
       "5 2015-07-18 02:00:00 2015-07-19 07:00:00            29.0  \n",
       "6 2010-11-13 15:00:00 2010-11-14 22:00:00            31.0  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_columns = [\n",
    "    'YEAR', 'MONTH', 'U.S._STATE', 'CLIMATE.REGION', 'ANOMALY.LEVEL',\n",
    "    'CLIMATE.CATEGORY', 'CUSTOMERS.AFFECTED', 'POPDEN_URBAN', 'POPDEN_RURAL',\n",
    "    'AREAPCT_URBAN', 'AREAPCT_UC', 'PCT_LAND', 'PCT_WATER_TOT', 'PCT_WATER_INLAND',\n",
    "    'OUTAGE.START', 'OUTAGE.RESTORATION', 'DURATION_HOURS'\n",
    "]\n",
    "\n",
    "df_filtered = df_valid[relevant_columns].copy()\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.box(df_valid, y=\"DURATION_HOURS\", title=\"Outage Duration Distribution (Box Plot)\")\n",
    "fig.write_html(\"univariate_duration_box.html\")\n",
    "\n",
    "# Histogram of CUSTOMERS.AFFECTED\n",
    "fig2 = px.histogram(df_valid, x=\"CUSTOMERS.AFFECTED\", nbins=50, title=\"Distribution of Customers Affected\")\n",
    "fig2.write_html(\"univariate_customers.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Univarite analysis :duration hours*\n",
    "\n",
    "This box plot illustrates the distribution of outage durations in hours. The median outage duration is approximately 21 hours, with most outages lasting under 60 hours. However, there are several extreme events exceeding 140 hours, which are visualized as outliers. These long outages may be due to severe weather or infrastructure failures and are rare but impactful. This insight supports our focus on duration as a key indicator of severity.\n",
    "\n",
    "*Univariate Analysis: Customers Affected (Histogram)*\n",
    "\n",
    "This histogram displays the distribution of the number of customers affected by each outage. The distribution is highly right-skewed — most outages affect fewer than 250,000 customers, while a few extreme cases impact over 1 million. These rare but massive events are important for understanding system vulnerability and planning mitigation efforts. The presence of such a long tail highlights the need for careful handling of this variable during model training, potentially via transformation or robust scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "state_coords = {\n",
    "    'Alabama': [32.806671, -86.791130],\n",
    "    'Alaska': [61.370716, -152.404419],\n",
    "    'Arizona': [33.729759, -111.431221],\n",
    "    'Arkansas': [34.969704, -92.373123],\n",
    "    'California': [36.116203, -119.681564],\n",
    "    'Colorado': [39.059811, -105.311104],\n",
    "    'Connecticut': [41.597782, -72.755371],\n",
    "    'Delaware': [39.318523, -75.507141],\n",
    "    'District of Columbia': [38.897438, -77.026817],\n",
    "    'Florida': [27.766279, -81.686783],\n",
    "    'Georgia': [33.040619, -83.643074],\n",
    "    'Hawaii': [21.094318, -157.498337],\n",
    "    'Idaho': [44.240459, -114.478828],\n",
    "    'Illinois': [40.349457, -88.986137],\n",
    "    'Indiana': [39.849426, -86.258278],\n",
    "    'Iowa': [42.011539, -93.210526],\n",
    "    'Kansas': [38.526600, -96.726486],\n",
    "    'Kentucky': [37.668140, -84.670067],\n",
    "    'Louisiana': [31.169546, -91.867805],\n",
    "    'Maine': [44.693947, -69.381927],\n",
    "    'Maryland': [39.063946, -76.802101],\n",
    "    'Massachusetts': [42.230171, -71.530106],\n",
    "    'Michigan': [43.326618, -84.536095],\n",
    "    'Minnesota': [45.694454, -93.900192],\n",
    "    'Mississippi': [32.741646, -89.678696],\n",
    "    'Missouri': [38.456085, -92.288368],\n",
    "    'Montana': [46.921925, -110.454353],\n",
    "    'Nebraska': [41.125370, -98.268082],\n",
    "    'Nevada': [38.313515, -117.055374],\n",
    "    'New Hampshire': [43.452492, -71.563896],\n",
    "    'New Jersey': [40.298904, -74.521011],\n",
    "    'New Mexico': [34.840515, -106.248482],\n",
    "    'New York': [42.165726, -74.948051],\n",
    "    'North Carolina': [35.630066, -79.806419],\n",
    "    'North Dakota': [47.528912, -99.784012],\n",
    "    'Ohio': [40.388783, -82.764915],\n",
    "    'Oklahoma': [35.565342, -96.928917],\n",
    "    'Oregon': [44.572021, -122.070938],\n",
    "    'Pennsylvania': [40.590752, -77.209755],\n",
    "    'Rhode Island': [41.680893, -71.511780],\n",
    "    'South Carolina': [33.856892, -80.945007],\n",
    "    'South Dakota': [44.299782, -99.438828],\n",
    "    'Tennessee': [35.747845, -86.692345],\n",
    "    'Texas': [31.054487, -97.563461],\n",
    "    'Utah': [40.150032, -111.862434],\n",
    "    'Vermont': [44.045876, -72.710686],\n",
    "    'Virginia': [37.769337, -78.169968],\n",
    "    'Washington': [47.400902, -121.490494],\n",
    "    'West Virginia': [38.491226, -80.954570],\n",
    "    'Wisconsin': [44.268543, -89.616508],\n",
    "    'Wyoming': [42.755966, -107.302490]\n",
    "}\n",
    "\n",
    "\n",
    "# Create a copy to avoid SettingWithCopyWarning\n",
    "df_valid = df_valid.copy()\n",
    "\n",
    "# Add latitude and longitude safely\n",
    "df_valid[\"LAT\"] = df_valid[\"U.S._STATE\"].map(lambda x: state_coords.get(x, [None, None])[0])\n",
    "df_valid[\"LON\"] = df_valid[\"U.S._STATE\"].map(lambda x: state_coords.get(x, [None, None])[1])\n",
    "\n",
    "# Drop rows with missing coordinates\n",
    "df_valid.dropna(subset=[\"LAT\", \"LON\"], inplace=True)\n",
    "\n",
    "# Create GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(df_valid, geometry=gpd.points_from_xy(df_valid[\"LON\"], df_valid[\"LAT\"]), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration_by_cause.html\n"
     ]
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "duration_by_cause = df_valid.groupby('CAUSE.CATEGORY')['DURATION_HOURS'].mean().sort_values(ascending=False).reset_index()\n",
    "\n",
    "fig2 = px.bar(\n",
    "    duration_by_cause,\n",
    "    x='CAUSE.CATEGORY',\n",
    "    y='DURATION_HOURS',\n",
    "    title='Average Outage Duration by Cause',\n",
    "    labels={'CAUSE.CATEGORY': 'Cause of Outage', 'DURATION_HOURS': 'Average Duration (Hours)'},\n",
    "    color='CAUSE.CATEGORY'\n",
    ")\n",
    "fig2.update_layout(xaxis_title=\"Cause of Outage\", yaxis_title=\"Average Duration (Hours)\")\n",
    "\n",
    "# Save the graph to an HTML file\n",
    "fig2.write_html(\"duration_by_cause.html\")\n",
    "print(\"duration_by_cause.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to project/outages_over_time.html\n"
     ]
    }
   ],
   "source": [
    "# Create a 'YearMonth' column for aggregation\n",
    "df_valid['YearMonth'] = df_valid['OUTAGE.START'].dt.to_period('M').astype(str)\n",
    "outages_over_time = df_valid.groupby('YearMonth').size().reset_index(name='count')\n",
    "\n",
    "fig1 = px.line(\n",
    "    outages_over_time,\n",
    "    x='YearMonth',\n",
    "    y='count',\n",
    "    title='Number of Power Outages Over Time',\n",
    "    labels={'YearMonth': 'Month', 'count': 'Number of Outages'},\n",
    "    markers=True\n",
    ")\n",
    "fig1.update_layout(xaxis_title=\"Month\", yaxis_title=\"Number of Outages\")\n",
    "\n",
    "# Save the graph to an HTML file to embed in your website\n",
    "fig1.write_html(\"outages_over_time.html\")\n",
    "print(\"Saved to project/outages_over_time.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Graph 4: Outage Distribution by Climate Region...\n",
      "Saved to project/outages_by_region.html\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGenerating Graph 4: Outage Distribution by Climate Region...\")\n",
    "outages_by_region = df_valid['CLIMATE.REGION'].value_counts().reset_index()\n",
    "outages_by_region.columns = ['CLIMATE.REGION', 'count']\n",
    "\n",
    "fig4 = px.treemap(\n",
    "    outages_by_region,\n",
    "    path=[px.Constant(\"All Regions\"), 'CLIMATE.REGION'],\n",
    "    values='count',\n",
    "    title='Proportion of Power Outages by Climate Region',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel\n",
    ")\n",
    "fig4.update_layout(margin = dict(t=50, l=25, r=25, b=25))\n",
    "\n",
    "# Save the graph to an HTML file\n",
    "fig4.write_html(\"outages_by_region.html\")\n",
    "print(\"Saved to project/outages_by_region.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import folium\n",
    "\n",
    "\n",
    "map = folium.Map(location=[37.8, -96], zoom_start=3)\n",
    "\n",
    "\n",
    "for _, row in df[df[\"U.S._STATE\"].notna() & df[\"CUSTOMERS.AFFECTED\"].notna()].iterrows():\n",
    "    state = row[\"U.S._STATE\"]\n",
    "    if state in state_coords:\n",
    "        affected = row[\"CUSTOMERS.AFFECTED\"]\n",
    "        \n",
    "        radius = min(30, max(4, affected**0.3 / 3)) \n",
    "        color = 'red' if affected > 500_000 else 'orange' if affected > 100_000 else 'green'\n",
    "        \n",
    "        folium.CircleMarker(\n",
    "            location=state_coords[state],\n",
    "            radius=radius,\n",
    "            color=color,\n",
    "            fill=True,\n",
    "            fill_opacity=0.7,\n",
    "            popup=f\"{state}<br>Cause: {row['CAUSE.CATEGORY']}<br>Affected: {int(affected):,}\"\n",
    "        ).add_to(map)\n",
    "\n",
    "\n",
    "with open(\"outages_map.html\", \"w\") as f:\n",
    "    f.write(map._repr_html_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = df_valid.pivot_table(\n",
    "    values=\"DURATION_HOURS\",\n",
    "    index=\"CLIMATE.REGION\",\n",
    "    columns=\"CLIMATE.CATEGORY\",\n",
    "    aggfunc=\"mean\"\n",
    ").round(1)\n",
    "\n",
    "pivot_html = pivot_df.to_html(classes=\"styled-table\", border=0)\n",
    "with open(\"pivot_table.html\", \"w\") as f:\n",
    "    f.write(pivot_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Framing a Prediction Problem\n",
    "Predict the cause of a major power outage.\n",
    "\n",
    "In this project, we aim to predict the cause of a major power outage in the United States using relevant environmental, regional, and temporal data available at the time of the outage. The goal is to assist utility companies and policy-makers in proactively identifying risk factors and preparing mitigation strategies.\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "•\tProblem Type: This is a multiclass classification task, as the target variable contains more than two distinct categories.\n",
    "•\tResponse Variable: CAUSE.CATEGORY — this column represents the high-level cause of the power outage (e.g., intentional attack, equipment failure, public appeal, etc.). We chose this variable because understanding the cause of an outage has practical value for prevention and planning.\n",
    "•\tEvaluation Metric: We use the macro-averaged F1-score to evaluate model performance. This metric is more appropriate than accuracy because our dataset is imbalanced, with some cause categories appearing much less frequently than others. Macro F1 equally weights each class, ensuring that rare but important categories are not ignored.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1-score (Baseline): 0.373\n",
      "\n",
      "Classification Report:\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "            equipment failure       0.00      0.00      0.00         5\n",
      "        fuel supply emergency       0.00      0.00      0.00         1\n",
      "           intentional attack       0.82      0.85      0.84        39\n",
      "                    islanding       0.60      0.43      0.50         7\n",
      "                public appeal       0.00      0.00      0.00         4\n",
      "               severe weather       0.87      0.94      0.90       140\n",
      "system operability disruption       0.38      0.38      0.38        16\n",
      "\n",
      "                     accuracy                           0.82       212\n",
      "                    macro avg       0.38      0.37      0.37       212\n",
      "                 weighted avg       0.77      0.82      0.79       212\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# Step 1: Select relevant columns\n",
    "features = [\n",
    "    'YEAR', 'MONTH', 'U.S._STATE', 'CLIMATE.REGION',\n",
    "    'ANOMALY.LEVEL', 'CLIMATE.CATEGORY',\n",
    "    'CUSTOMERS.AFFECTED', 'DURATION_HOURS'\n",
    "]\n",
    "target = 'CAUSE.CATEGORY'\n",
    "\n",
    "# Drop rows with missing target\n",
    "df_model = df_valid.dropna(subset=[target])[features + [target]].copy()\n",
    "\n",
    "# Step 2: Split data\n",
    "X = df_model[features]\n",
    "y = df_model[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Step 3: Define preprocessing\n",
    "categorical_features = ['U.S._STATE', 'CLIMATE.REGION', 'CLIMATE.CATEGORY']\n",
    "numerical_features = ['YEAR', 'MONTH', 'ANOMALY.LEVEL', 'CUSTOMERS.AFFECTED', 'DURATION_HOURS']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 4: Build pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, solver='lbfgs'))\n",
    "])\n",
    "\n",
    "# Step 5: Train\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(\"Macro F1-score (Baseline):\", round(macro_f1, 3))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier__max_depth': 15, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 50}\n",
      "\n",
      "Classification Report:\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "            equipment failure       0.00      0.00      0.00         5\n",
      "        fuel supply emergency       0.00      0.00      0.00         1\n",
      "           intentional attack       0.93      0.95      0.94        39\n",
      "                    islanding       0.80      0.57      0.67         7\n",
      "                public appeal       0.00      0.00      0.00         4\n",
      "               severe weather       0.87      0.98      0.92       140\n",
      "system operability disruption       0.40      0.25      0.31        16\n",
      "\n",
      "                     accuracy                           0.86       212\n",
      "                    macro avg       0.43      0.39      0.40       212\n",
      "                 weighted avg       0.80      0.86      0.83       212\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "df_valid[\"YEAR\"] = df_valid[\"YEAR\"].astype(int)\n",
    "df_valid[\"MONTH\"] = pd.to_numeric(df_valid[\"MONTH\"], errors=\"coerce\")\n",
    "df_valid[\"CUSTOMERS.AFFECTED\"] = pd.to_numeric(df_valid[\"CUSTOMERS.AFFECTED\"], errors=\"coerce\")\n",
    "df_valid[\"ANOMALY.LEVEL\"] = pd.to_numeric(df_valid[\"ANOMALY.LEVEL\"], errors=\"coerce\")\n",
    "\n",
    "\n",
    "required_cols = [\"YEAR\", \"MONTH\", \"CUSTOMERS.AFFECTED\", \"ANOMALY.LEVEL\", \"CAUSE.CATEGORY\"]\n",
    "df_clean = df_valid.dropna(subset=required_cols).copy()\n",
    "df_clean = df_clean[df_clean[\"CUSTOMERS.AFFECTED\"] >= 0]\n",
    "df_clean = df_clean[df_clean[\"ANOMALY.LEVEL\"].notna()]\n",
    "\n",
    "def extract_season(X):\n",
    "    month = X[\"MONTH\"].astype(int)\n",
    "    return pd.DataFrame(np.select(\n",
    "        [month.isin([12,1,2]), month.isin([3,4,5]), month.isin([6,7,8]), month.isin([9,10,11])],\n",
    "        ['Winter', 'Spring', 'Summer', 'Fall'],\n",
    "        default='Unknown'\n",
    "    ), columns=[\"SEASON\"])\n",
    "\n",
    "def safe_log1p(X):\n",
    "    return np.log1p(np.where(X < 0, 0, X))  # Replace negative values with 0\n",
    "\n",
    "# === Features and target ===\n",
    "categorical = [\"U.S._STATE\", \"CLIMATE.CATEGORY\"]\n",
    "numerical = [\"YEAR\", \"MONTH\", \"CUSTOMERS.AFFECTED\", \"ANOMALY.LEVEL\"]\n",
    "X = df_clean[categorical + numerical]\n",
    "y = df_clean[\"CAUSE.CATEGORY\"]\n",
    "\n",
    "# === Train/Test Split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42, test_size=0.2)\n",
    "\n",
    "# === Preprocessing ===\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical),\n",
    "    (\"season\", Pipeline([\n",
    "        (\"season_gen\", FunctionTransformer(extract_season, validate=False)),\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ]), [\"MONTH\"]),\n",
    "    (\"num\", Pipeline([\n",
    "        (\"log\", FunctionTransformer(safe_log1p, validate=False)),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), numerical)\n",
    "])\n",
    "\n",
    "# === Final Model Pipeline ===\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"classifier\", RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# === Grid Search ===\n",
    "param_grid = {\n",
    "    \"classifier__n_estimators\": [50, 100],\n",
    "    \"classifier__max_depth\": [5, 10, 15],\n",
    "    \"classifier__min_samples_split\": [2, 5]\n",
    "}\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring=\"f1_macro\", n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# === Evaluation ===\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
